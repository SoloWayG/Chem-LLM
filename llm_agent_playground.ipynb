{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szRIMGTc4nLe"
      },
      "source": [
        "# Пример с ollama\n",
        "## Игрушечный пример на понимание интерфейса ChatOllama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LteIPGwT4nLj"
      },
      "source": [
        "Сначала устанавливаем ollama:\n",
        "```bash\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "```\n",
        "Можно через докер:\n",
        "https://hub.docker.com/r/ollama/ollama\n",
        "\n",
        "Затем через ollama загружаем нужную модель (или подаем свою в формате gguf):\n",
        "\n",
        "```bash\n",
        "ollama run llama3\n",
        "```\n",
        "\n",
        "Как подать свою есть инструкция на [гитхабе](https://github.com/ollama/ollama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFtLYvrcvxpp",
        "outputId": "1849f93b-ab1a-4aec-9aa6-b5e9986ba167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pci.ids all 0.0~2022.01.22-1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 2s (197 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install pciutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faVVoGgKrfSF",
        "outputId": "f7fd2547-1c57-411a-b1a4-8a269004bd46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6irNwIBEsZox",
        "outputId": "35e7734a-6ce5-45cf-c315-0f31860162a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024/06/05 09:34:27 routes.go:1007: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST: OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS: OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]\"\n",
            "time=2024-06-05T09:34:27.052Z level=INFO source=images.go:729 msg=\"total blobs: 0\"\n",
            "time=2024-06-05T09:34:27.053Z level=INFO source=images.go:736 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-06-05T09:34:27.053Z level=INFO source=routes.go:1053 msg=\"Listening on 127.0.0.1:11434 (version 0.1.41)\"\n",
            "time=2024-06-05T09:34:27.053Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama4287817558/runners\n",
            "time=2024-06-05T09:34:33.391Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60002]\"\n",
            "time=2024-06-05T09:34:33.541Z level=INFO source=types.go:71 msg=\"inference compute\" id=GPU-9324ee13-8166-2e2e-f134-80d00062df8d library=cuda compute=7.5 driver=12.2 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n"
          ]
        }
      ],
      "source": [
        "!ollama serve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-YuzH-z4nLk"
      },
      "source": [
        "### Пример для простого инференса из модели в Ollama с подстановкой аргумента:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzXwXkg84nLl",
        "outputId": "dc344bea-9326-437a-d642-46451690bebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**\"LLMs: The Game-Changers for Shipping Magnates Like Me\"**\n",
            "\n",
            "As a shipping magnate, I've spent my fair share of time navigating the complexities of global trade and logistics. From optimizing routes to managing inventory, every decision counts when it comes to keeping my fleet of vessels running smoothly and efficiently. But in recent years, I've noticed a significant shift in the way I do business – and it's all thanks to Large Language Models (LLMs).\n",
            "\n",
            "For those who may not be familiar, LLMs are artificial intelligence systems that can process and generate human-like language. They're essentially super-smart chatbots that can learn from vast amounts of data and respond accordingly. And let me tell you, they've revolutionized the way I operate my shipping empire.\n",
            "\n",
            "First and foremost, LLMs have streamlined our communication channels with customers and partners. Gone are the days of tedious phone calls and lengthy emails; now, we can simply converse with these AI-powered language models to get answers to our questions or resolve issues in real-time. It's like having a personal assistant at my beck and call – 24/7!\n",
            "\n",
            "But that's not all. LLMs have also enabled us to automate many of the mundane tasks that used to consume so much of our time. For instance, we can now use these AI models to analyze market trends, predict demand, and optimize inventory levels. It's like having a team of expert analysts working tirelessly behind the scenes to help me make informed decisions.\n",
            "\n",
            "And then there's the matter of documentation – always a thorn in the side of any shipping magnate. With LLMs, we can generate accurate, compliant documents (think bills of lading, customs forms, and the like) with ease. No more tedious typing or worrying about errors; these AI models are trained to get it right every time.\n",
            "\n",
            "But perhaps most excitingly, LLMs have opened up new opportunities for us to explore in terms of data analysis and predictive modeling. By feeding them vast amounts of historical data on shipping patterns, weather conditions, and market fluctuations, we can gain valuable insights into what's driving demand and how best to adapt our operations to meet those demands.\n",
            "\n",
            "For example, by analyzing LLM-generated forecasts on global trade trends, I can better anticipate which routes will be in high demand during peak seasons. This allows me to adjust my fleet's deployment accordingly, ensuring that we're always running at maximum efficiency.\n",
            "\n",
            "Of course, there are still some who might view LLMs with skepticism – and rightfully so. After all, these AI models are only as good as the data they're trained on, and there's always a risk of bias or errors creeping in. But I've found that the benefits far outweigh the risks when it comes to streamlining our operations and making more informed decisions.\n",
            "\n",
            "In conclusion, LLMs have been a game-changer for my shipping empire – and I'm not alone. As the industry continues to evolve at breakneck speed, I believe these AI-powered language models will only become more integral to our daily operations. So if you're a fellow shipping magnate looking to stay ahead of the curve, I highly recommend giving LLMs a try. Trust me, your bottom line (and your sanity) will thank you!"
          ]
        }
      ],
      "source": [
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Local Llama3\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3\",\n",
        "    keep_alive=-1,  # Модель не будет выгружаться\n",
        "    temperature=0,\n",
        "    max_new_tokens=512,\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write me a 500 word article on {topic} from the perspective of a {profession}. \"\n",
        ")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# print(chain.invoke({\"topic\": \"LLMs\", \"profession\": \"shipping magnate\"}))\n",
        "\n",
        "for chunk in chain.stream({\"topic\": \"LLMs\", \"profession\": \"shipping magnate\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLY7T_844nLn"
      },
      "source": [
        "### Небольшой пример, как работать с OllamaFunctions, как подавать в нее инструменты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooFSAd1O4nLo",
        "outputId": "88de5e0f-62ce-4096-b3e8-482b3ae0c07d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Singapore\", \"unit\": \"celsius\"}'}} id='run-4cea5090-8a87-410a-9a40-257903e68985-0'\n"
          ]
        }
      ],
      "source": [
        "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
        "\n",
        "model = OllamaFunctions(model=\"llama3\", format=\"json\")\n",
        "\n",
        "model = model.bind_tools(\n",
        "    tools=[\n",
        "        {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, \" \"e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        }\n",
        "    ],\n",
        "    function_call={\n",
        "        \"name\": \"get_current_weather\"\n",
        "    },  # Этот параметр ЗАСТАВЛЯЕТ модель использовать функцию ОБЯЗАТЕЛЬНО\n",
        ")\n",
        "\n",
        "response = model.invoke(\"what is the weather in Singapore?\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A75Z_Hdu4nLo"
      },
      "source": [
        "### Пример взаимодействия агента с инструментами, которые могут быть в stairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dajvbpsl4nLr"
      },
      "source": [
        "Задаем гипотетические инструменты для агента:\n",
        "1. Модуль моделирования зависимостей между работами и\n",
        "ресурсами\n",
        "2. Модуль восстановления связей м/у работами, иерархии работ и объектов\n",
        "3. Модуль автоматической генерации оптимальных планов\n",
        "4. Обращение к Базе данных для получения контекста с помощью RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-TBlXW54nLr"
      },
      "outputs": [],
      "source": [
        "dependency_modeling_tool = {\n",
        "    \"name\": \"dependency_modeling\",\n",
        "    \"description\": \"Model dependencies between tasks and resources\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"tasks\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"List of tasks to model dependencies for\",\n",
        "            },\n",
        "            \"resources\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"List of resources involved in the tasks\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"tasks\", \"resources\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "relationship_recovery_tool = {\n",
        "    \"name\": \"relationship_recovery\",\n",
        "    \"description\": \"Recover relationships between tasks, task hierarchies, and objects\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"task_list\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"List of tasks to analyze\",\n",
        "            },\n",
        "            \"hierarchy_levels\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"Number of hierarchy levels to consider\",\n",
        "            },\n",
        "            \"object_types\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"Types of objects to include in the recovery\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"task_list\", \"hierarchy_levels\", \"object_types\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "optimal_planning_tool = {\n",
        "    \"name\": \"optimal_planning\",\n",
        "    \"description\": \"Automatically generate optimal plans\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"constraints\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"List of constraints to consider in planning\",\n",
        "            },\n",
        "            \"objectives\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"List of objectives to achieve in the plan\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"constraints\", \"objectives\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "database_query_tool = {\n",
        "    \"name\": \"query_database_rag\",\n",
        "    \"description\": \"Query the database to retrieve context using Retrieval-Augmented Generation (RAG)\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"query\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The query to search the database\",\n",
        "            },\n",
        "            \"context_length\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"The length of context to retrieve\",\n",
        "                \"default\": 5,\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"query\"],\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRYt1IAQ4nLs"
      },
      "source": [
        "Отдаем инструменты модели и задаем наводящие вопросы, чтобы получить вызов инструмента, получаем вызов в переменной additional_kwargs, далее все это можно вытаскивать из словаря и исполнять"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoJwytVR4nLt",
        "outputId": "720a07a2-2fa1-4417-ccba-2760bc14c97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' additional_kwargs={'function_call': {'name': 'dependency_modeling', 'arguments': '{\"tasks\": [\"Excavation\", \"Foundation\", \"Framing\"], \"resources\": [\"Crane\", \"Crew A\"]}'}} id='run-db307d2e-9269-4793-baae-bf087e91ed3a-0'\n"
          ]
        }
      ],
      "source": [
        "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
        "\n",
        "model = OllamaFunctions(model=\"llama3\", format=\"json\")\n",
        "\n",
        "model = model.bind_tools(\n",
        "    tools=[\n",
        "        dependency_modeling_tool,\n",
        "        relationship_recovery_tool,\n",
        "        optimal_planning_tool,\n",
        "        database_query_tool,\n",
        "    ],\n",
        ")\n",
        "\n",
        "response_dependencies = model.invoke(\n",
        "    \"Can you model the dependencies between tasks 'Excavation', 'Foundation', and 'Framing' using resources 'Crane' and 'Crew A'?\"\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk8-B14j4nLt",
        "outputId": "ffba5bf6-492b-4ba5-a3be-8aa78f3ec5b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'relationship_recovery', 'arguments': '{\"task_list\": [\"Excavation\", \"Foundation\", \"Framing\"], \"hierarchy_levels\": 3, \"object_types\": [\"Site\", \"Materials\"]}'}}, id='run-65255b18-2042-4dd7-868c-e570d6e4ab97-0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_recovery = model.invoke(\n",
        "    \"Recover the relationships between tasks 'Excavation', 'Foundation', and 'Framing', considering 3 levels of hierarchy and including objects 'Site', 'Materials'.\"\n",
        ")\n",
        "response_recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip1tzFjy4nLu",
        "outputId": "683bf0ad-496b-48ec-b020-9a50cd873932"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'optimal_planning', 'arguments': '{\"type\": \"object\", \"properties\": {\"constraints\": [{\"name\": \"budget\", \"value\": 5000000, \"unit\": \"rubbles\"}, {\"name\": \"time\", \"value\": 10, \"unit\": \"days\"}], \"objectives\": [{\"name\": \"minimum cost\", \"weight\": 1}]}}'}}, id='run-a44570fb-d183-4fa5-9b80-ad523a3afb9d-0')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_plan = model.invoke(\n",
        "    \"Generate an optimal plan considering constraints 'budget' is 5 mil rubles, 'time' 10 days and objective 'minimum cost'.\"\n",
        ")\n",
        "response_plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSgwM4ID4nLu",
        "outputId": "e5a72417-8a7e-452b-912b-7733e55c28e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'query_database_rag', 'arguments': '{\"query\": \"how to build a dormitory\", \"context_length\": 5}'}}, id='run-a38cb31f-a8a5-49bf-8311-200307f15b9f-0')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_rag = model.invoke(\n",
        "    \"Query the database for the latest project reports using the query 'how to build a dormitory' and retrieve 5 context entries.\"\n",
        ")\n",
        "response_rag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GxZSd-j4nLu"
      },
      "source": [
        "### Structured output с помощью Ollama\n",
        "Возможно, тоже может быть полезно для получения инфы из БД, чтобы приводить ее в определенный вид"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5pA6N7J4nLu"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
        "\n",
        "\n",
        "# Pydantic Schema for structured response\n",
        "class Person(BaseModel):\n",
        "    name: str = Field(description=\"The person's name\", required=True)\n",
        "    height: float = Field(description=\"The person's height\", required=True)\n",
        "    hair_color: str = Field(description=\"The person's hair color\")\n",
        "\n",
        "\n",
        "context = \"\"\"Alex is 5 feet tall.\n",
        "Claudia is 1 feet taller than Alex and jumps higher than him.\n",
        "Claudia is a brunette and Alex is blonde.\"\"\"\n",
        "\n",
        "# Prompt template llama3\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "    You are a smart assistant take the following context and question below and return your answer in JSON.\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "QUESTION: {question} \\n\n",
        "CONTEXT: {context} \\n\n",
        "JSON:\n",
        "<|eot_id|>\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        " \"\"\"\n",
        ")\n",
        "\n",
        "# Chain\n",
        "llm = OllamaFunctions(model=\"llama3\", format=\"json\", temperature=0)\n",
        "\n",
        "structured_llm = llm.with_structured_output(Person)\n",
        "chain = prompt | structured_llm\n",
        "\n",
        "response = chain.invoke({\"question\": \"Who is taller?\", \"context\": context})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdLmz2B94nLv"
      },
      "source": [
        "## Вариации без Ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GPhNRZ44nLv"
      },
      "source": [
        "### Простые примеры:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-apG6zx4nLv"
      },
      "source": [
        "Первый вариант подходит для любой модели, но остается только надеяться, что модель в каком-то виде отдаст то, что нужно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7c433d1038e44788afc4a9b524c71150"
          ]
        },
        "id": "GcN6cABS4nLv",
        "outputId": "650b9f4a-a58c-406b-a2d7-b851b79b10ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c433d1038e44788afc4a9b524c71150",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "/home/roman/Documents/jerzy/llm-agents/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "do:  Use example_tool to solve this. Example tool is a tool that can be used to solve a\n",
            "Tool called successfully!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "class ToolCallingModel:\n",
        "    def __init__(self, model_name):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tools = {}\n",
        "\n",
        "    def add_tool(self, tool_name, tool_function):\n",
        "        self.tools[tool_name] = tool_function\n",
        "\n",
        "    def call_model(self, input_text):\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        outputs = self.model.generate(**inputs)\n",
        "        decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(\"do: \", decoded_output)\n",
        "        return self._check_for_tool_use(decoded_output)\n",
        "\n",
        "    def _check_for_tool_use(self, model_output):\n",
        "        for tool_name, tool_function in self.tools.items():\n",
        "            if tool_name in model_output:\n",
        "                # Extract relevant data and call the tool function\n",
        "                return tool_function()\n",
        "        return model_output\n",
        "\n",
        "\n",
        "# Example tool function\n",
        "def example_tool():\n",
        "    return \"Tool called successfully!\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = ToolCallingModel(\"mzbac/llama-3-8B-Instruct-function-calling\")\n",
        "    model.add_tool(\"example_tool\", example_tool)\n",
        "    response = model.call_model(\"Use example_tool to solve this.\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BywTM0L4nLw"
      },
      "source": [
        "Второй вариант более интересный, модель mzbac/llama-3-8B-Instruct-function-calling уже предобучена на glaive-function-calling-v2, еще существует такой датасет https://huggingface.co/datasets/danilopeixoto/pandora-tool-calling для предобучения. Такая модель будет выдвать function calling в том же виде, после токена \\<functioncall\\>, что в целом нам тоже подходит"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1728b77f7f084f48adb6edc05ffdfaea"
          ]
        },
        "id": "wl44EGVs4nLw",
        "outputId": "bc18fd8b-015e-43d6-c7c5-21687b42d058"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1728b77f7f084f48adb6edc05ffdfaea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant with access to the following functions. Use them if required - {'name':'search_web', 'description': 'Perform a web search for a given search terms.', 'parameter': {'type': 'object', 'properties': {'search_terms': {'type': 'array', 'items': {'type':'string'}, 'description': 'The search queries for which the search is performed.','required': True}}}}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Today's news in Melbourne, just for your information, today is April 27, 2014.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<functioncall> {\"name\": \"search_web\", \"arguments\": '{\"search_terms\": [\"Melbourne news\", \"April 27, 2014\"]}'}<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"mzbac/llama-3-8B-Instruct-function-calling\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tool = {\n",
        "    \"name\": \"search_web\",\n",
        "    \"description\": \"Perform a web search for a given search terms.\",\n",
        "    \"parameter\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"search_terms\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"The search queries for which the search is performed.\",\n",
        "                \"required\": True,\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": f\"You are a helpful assistant with access to the following functions. Use them if required - {str(tool)}\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Today's news in Melbourne, just for your information, today is April 27, 2014.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        ")\n",
        "response = outputs[0]\n",
        "print(tokenizer.decode(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBVxNCBp4nLx"
      },
      "source": [
        "### Пример с теми же функциями stairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "76511116f8a349cdbbbdf8292716af5c"
          ]
        },
        "id": "DViElIGS4nLx",
        "outputId": "a01fcc8a-7ffa-4ac0-ac25-626b25f90b13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76511116f8a349cdbbbdf8292716af5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant with access to the following functions. Use them if required - [{'name': 'dependency_modeling', 'description': 'Model dependencies between tasks and resources', 'parameters': {'type': 'object', 'properties': {'tasks': {'type': 'array', 'items': {'type':'string'}, 'description': 'List of tasks to model dependencies for'},'resources': {'type': 'array', 'items': {'type':'string'}, 'description': 'List of resources involved in the tasks'}},'required': ['tasks','resources']}}, {'name':'relationship_recovery', 'description': 'Recover relationships between tasks, task hierarchies, and objects', 'parameters': {'type': 'object', 'properties': {'task_list': {'type': 'array', 'items': {'type':'string'}, 'description': 'List of tasks to analyze'}, 'hierarchy_levels': {'type': 'integer', 'description': 'Number of hierarchy levels to consider'}, 'object_types': {'type': 'array', 'items': {'type':'string'}, 'description': 'Types of objects to include in the recovery'}},'required': ['task_list', 'hierarchy_levels', 'object_types']}}, {'name': 'optimal_planning', 'description': 'Automatically generate optimal plans', 'parameters': {'type': 'object', 'properties': {'constraints': {'type': 'array', 'items': {'type':'string'}, 'description': 'List of constraints to consider in planning'}, 'objectives': {'type': 'array', 'items': {'type':'string'}, 'description': 'List of objectives to achieve in the plan'}},'required': ['constraints', 'objectives']}}, {'name': 'query_database_rag', 'description': 'Query the database to retrieve context using Retrieval-Augmented Generation (RAG)', 'parameters': {'type': 'object', 'properties': {'query': {'type':'string', 'description': 'The query to search the database'}, 'context_length': {'type': 'integer', 'description': 'The length of context to retrieve', 'default': 5}},'required': ['query']}}]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Can you model the dependencies between tasks 'Excavation', 'Foundation', and 'Framing' using resources 'Crane' and 'Crew A'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<functioncall> {\"name\": \"dependency_modeling\", \"arguments\": '{\"tasks\": [\"Excavation\", \"Foundation\", \"Framing\"], \"resources\": [\"Crane\", \"Crew A\"]}'}<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"mzbac/llama-3-8B-Instruct-function-calling\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tools = [\n",
        "    dependency_modeling_tool,\n",
        "    relationship_recovery_tool,\n",
        "    optimal_planning_tool,\n",
        "    database_query_tool,\n",
        "]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": f\"You are a helpful assistant with access to the following functions. Use them if required - {str(tools)}\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Can you model the dependencies between tasks 'Excavation', 'Foundation', and 'Framing' using resources 'Crane' and 'Crew A'?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        ")\n",
        "response = outputs[0]\n",
        "print(tokenizer.decode(response))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
